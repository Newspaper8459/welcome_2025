{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d7eea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from continuum.datasets import CIFAR10, ImageFolderDataset\n",
    "from continuum.scenarios import ClassIncremental\n",
    "from continuum.metrics import Logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac012653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "  dataset = 'cifar-10'\n",
    "  increment = 2\n",
    "  initial_increment = 2\n",
    "  log_path = Path('log') / f'{dataset}_{initial_increment}_{increment}'\n",
    "  batch_size_train = 128\n",
    "  batch_size_valid = 128\n",
    "\n",
    "  num_epochs = 15\n",
    "\n",
    "cfg = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c2f0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "  \"\"\"Fix all random seeds\"\"\"\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  os.environ['PYTHONHASHSEED'] = str(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc54dc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncrementalResNet18(nn.Module):\n",
    "  def __init__(self, *args, **kwargs) -> None:\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.backbone = models.resnet18()\n",
    "    # self.transforms = models.ResNet18_Weights.IMAGENET1K_V1.transforms\n",
    "\n",
    "    self.backbone.fc = nn.Linear(self.backbone.fc.in_features, 0)\n",
    "    self.num_classes = 0\n",
    "\n",
    "  def adaptation(self, increment: int) -> None:\n",
    "    old_fc = self.backbone.fc\n",
    "    in_features = old_fc.in_features\n",
    "\n",
    "    new_fc = nn.Linear(in_features, self.num_classes + increment).to(cfg.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      new_fc.weight[:-increment] = old_fc.weight.detach().clone()\n",
    "\n",
    "    self.backbone.fc = new_fc\n",
    "    self.num_classes += increment\n",
    "\n",
    "  def forward(self, x) -> None:\n",
    "    x = self.backbone(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24c1bfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/misaizu/welcome_2025/backend/.venv/lib/python3.11/site-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "model = IncrementalResNet18().to(cfg.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08c8551",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = CIFAR10(data_path='input', train=True, download=True)\n",
    "dataset_valid = CIFAR10(data_path='input', train=False, download=True)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "  transforms.Resize(224),\n",
    "  transforms.CenterCrop(224),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225]\n",
    "  )\n",
    "])\n",
    "\n",
    "scenario_train = ClassIncremental(dataset_train, increment=2, initial_increment=2, transformations=preprocess.transforms)\n",
    "scenario_valid = ClassIncremental(dataset_valid, increment=2, initial_increment=2, transformations=preprocess.transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5956f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.log_path.mkdir(parents=True, exist_ok=True)\n",
    "with open(cfg.log_path / 'metrics.json', 'w') as f:\n",
    "  pass\n",
    "\n",
    "metric_logger = Logger(list_subsets=['test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c8610a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/15 | Loss: 0.4973:  90%|████████▉ | 71/79 [00:12<00:01,  5.63it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m loss = torch.tensor(\u001b[32m0.0\u001b[39m).to(cfg.device)\n\u001b[32m     17\u001b[39m tqdm_loader = tqdm(dataloader_train)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m  \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m  \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/misaizu/welcome_2025/backend/.venv/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/misaizu/welcome_2025/backend/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/misaizu/welcome_2025/backend/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/misaizu/welcome_2025/backend/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/misaizu/welcome_2025/backend/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/misaizu/welcome_2025/backend/.venv/lib/python3.11/site-packages/continuum/tasks/image_array_task_set.py:102\u001b[39m, in \u001b[36mArrayTaskSet.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     94\u001b[39m     bbox = \u001b[38;5;28mself\u001b[39m.bounding_boxes[index]\n\u001b[32m     95\u001b[39m     x = x.crop((\n\u001b[32m     96\u001b[39m         \u001b[38;5;28mmax\u001b[39m(bbox[\u001b[32m0\u001b[39m], \u001b[32m0\u001b[39m),  \u001b[38;5;66;03m# x1\u001b[39;00m\n\u001b[32m     97\u001b[39m         \u001b[38;5;28mmax\u001b[39m(bbox[\u001b[32m1\u001b[39m], \u001b[32m0\u001b[39m),  \u001b[38;5;66;03m# y1\u001b[39;00m\n\u001b[32m     98\u001b[39m         \u001b[38;5;28mmin\u001b[39m(bbox[\u001b[32m2\u001b[39m], x.size[\u001b[32m0\u001b[39m]),  \u001b[38;5;66;03m# x2\u001b[39;00m\n\u001b[32m     99\u001b[39m         \u001b[38;5;28mmin\u001b[39m(bbox[\u001b[32m3\u001b[39m], x.size[\u001b[32m1\u001b[39m]),  \u001b[38;5;66;03m# y2\u001b[39;00m\n\u001b[32m    100\u001b[39m     ))\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m x, y, t = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_trsf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    105\u001b[39m     y = \u001b[38;5;28mself\u001b[39m.get_task_target_trsf(t)(y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/misaizu/welcome_2025/backend/.venv/lib/python3.11/site-packages/continuum/tasks/image_array_task_set.py:111\u001b[39m, in \u001b[36mArrayTaskSet._prepare_data\u001b[39m\u001b[34m(self, x, y, t)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prepare_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y, t):\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trsf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m         x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_task_trsf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch.Tensor):\n\u001b[32m    113\u001b[39m         x = \u001b[38;5;28mself\u001b[39m._to_tensor(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/misaizu/welcome_2025/backend/.venv/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/misaizu/welcome_2025/backend/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/misaizu/welcome_2025/backend/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/misaizu/welcome_2025/backend/.venv/lib/python3.11/site-packages/torchvision/transforms/transforms.py:386\u001b[39m, in \u001b[36mCenterCrop.forward\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m    379\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    380\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    381\u001b[39m \u001b[33;03m        img (PIL Image or Tensor): Image to be cropped.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    384\u001b[39m \u001b[33;03m        PIL Image or Tensor: Cropped image.\u001b[39;00m\n\u001b[32m    385\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcenter_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/misaizu/welcome_2025/backend/.venv/lib/python3.11/site-packages/torchvision/transforms/functional.py:594\u001b[39m, in \u001b[36mcenter_crop\u001b[39m\u001b[34m(img, output_size)\u001b[39m\n\u001b[32m    592\u001b[39m crop_top = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m((image_height - crop_height) / \u001b[32m2.0\u001b[39m))\n\u001b[32m    593\u001b[39m crop_left = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m((image_width - crop_width) / \u001b[32m2.0\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m594\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_top\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_left\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_width\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/misaizu/welcome_2025/backend/.venv/lib/python3.11/site-packages/torchvision/transforms/functional.py:548\u001b[39m, in \u001b[36mcrop\u001b[39m\u001b[34m(img, top, left, height, width)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcrop\u001b[39m(img: Tensor, top: \u001b[38;5;28mint\u001b[39m, left: \u001b[38;5;28mint\u001b[39m, height: \u001b[38;5;28mint\u001b[39m, width: \u001b[38;5;28mint\u001b[39m) -> Tensor:\n\u001b[32m    532\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Crop the given image at specified location and output size.\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[33;03m    If the image is torch Tensor, it is expected\u001b[39;00m\n\u001b[32m    534\u001b[39m \u001b[33;03m    to have [..., H, W] shape, where ... means an arbitrary number of leading dimensions.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    545\u001b[39m \u001b[33;03m        PIL Image or Tensor: Cropped image.\u001b[39;00m\n\u001b[32m    546\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_tracing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    549\u001b[39m         _log_api_usage_once(crop)\n\u001b[32m    550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.Tensor):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/misaizu/welcome_2025/backend/.venv/lib/python3.11/site-packages/torch/jit/_trace.py:1329\u001b[39m, in \u001b[36mis_tracing\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_scripting():\n\u001b[32m   1328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_is_tracing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for task_id in range(len(scenario_valid)):\n",
    "  logging.info(f'Train for task {task_id} has started.')\n",
    "  model.adaptation(cfg.initial_increment if task_id == 0 else cfg.increment)\n",
    "\n",
    "  dataloader_train = DataLoader(scenario_train[task_id], batch_size=cfg.batch_size_train, shuffle=True)\n",
    "  dataloader_valid = DataLoader(scenario_valid[:task_id+1], batch_size=cfg.batch_size_valid)\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  optimizer = optim.AdamW(params=model.parameters())\n",
    "  scheduler = lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=cfg.num_epochs)\n",
    "\n",
    "  for i_epoch in range(cfg.num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = torch.tensor(0.0).to(cfg.device)\n",
    "    tqdm_loader = tqdm(dataloader_train)\n",
    "\n",
    "    for X, y, task_ids in tqdm_loader:\n",
    "      X, y = X.to(cfg.device), y.to(cfg.device)\n",
    "\n",
    "      y_pred = model(X)\n",
    "\n",
    "      loss = F.cross_entropy(y_pred, y)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      scheduler.step()\n",
    "\n",
    "      tqdm_loader.set_description(f'Epoch: {i_epoch+1}/{cfg.num_epochs} | Loss: {loss.item():.4f}')\n",
    "\n",
    "  tqdm_loader = tqdm(dataloader_valid)\n",
    "  model.eval()\n",
    "  for X, y, task_ids in tqdm_loader:\n",
    "    X = X.to(cfg.device)\n",
    "\n",
    "    y_pred = model(X)\n",
    "    y_pred = F.softmax(y_pred, dim=0)\n",
    "    metric_logger.add([y_pred.cpu().argmax(dim=1), y, task_ids], subset='test')\n",
    "\n",
    "  with open(cfg.log_path / 'metrics.json', 'r') as f:\n",
    "    if task_id == 0:\n",
    "      d = {}\n",
    "    else:\n",
    "      d = json.load(f)\n",
    "  with open(cfg.log_path / 'metrics.json', 'w') as f:\n",
    "    d[f'task_{task_id}'] = {\n",
    "      'task': task_id,\n",
    "      'acc': round(100 * metric_logger.accuracy, 2),\n",
    "      'avg_acc': round(100 * metric_logger.average_incremental_accuracy, 2),\n",
    "      'forgetting': round(100 * metric_logger.forgetting, 6),\n",
    "      'acc_per_task': [round(100 * acc_t, 2) for acc_t in metric_logger.accuracy_per_task],\n",
    "      'bwt': round(100 * metric_logger.backward_transfer, 2),\n",
    "      'fwt': round(100 * metric_logger.forward_transfer, 2),\n",
    "    }\n",
    "\n",
    "    json.dump(d, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
